# -*- coding: utf-8 -*-
"""Ajeet.py

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DK_LPkw9vXx7keETLfzlU_PF7FlHY3Tb
"""

import csv
import requests
from bs4 import BeautifulSoup

def get_product_data(url):
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}
    response = requests.get(url, headers=headers)
    soup = BeautifulSoup(response.text, 'html.parser')
    products = []

    for product in soup.select('.s-result-item'):
        product_url = product.select_one('.a-link-normal')['href']
        product_name = product.select_one('.a-text-normal').text.strip()
        product_price = product.select_one('.a-offscreen').text.strip()
        rating = product.select_one('.a-icon-star span').text.strip()
        num_reviews = product.select_one('.a-size-base').text.strip()

        products.append({
            'Product URL': product_url,
            'Product Name': product_name,
            'Product Price': product_price,
            'Rating': rating,
            'Number of Reviews': num_reviews
        })

    return products

def scrape_multiple_pages(num_pages):
    base_url = 'https://www.amazon.in/s?k=bags&crid=2M096C61O4MLT&qid=1653308124&sprefix=ba%2Caps%2C283&ref=sr_pg_{}'
    all_products = []

    for page in range(1, num_pages + 1):
        url = base_url.format(page)
        products_on_page = get_product_data(url)
        all_products.extend(products_on_page)

    return all_products

# Scrape 20 pages of product listing data
num_pages_to_scrape = 20
data_part_1 = scrape_multiple_pages(num_pages_to_scrape)

# Now, move on to Part 2 where you'll hit each URL and get more product details.

# To be continued in the next response...

